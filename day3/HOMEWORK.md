# Day3任意課題 :  独自の質問と参照文書を用いる実験、RAGの効果を定量的・定性的に評価 - 第二次トランプ政権の関税（表データ）に関するRAGの効果検証

このドキュメントは、Wikipediaの特定の表データを用いたRetrieval Augmented Generation (RAG) の効果検証に関する宿題の提出物です。
（https://ja.wikipedia.org/wiki/%E7%AC%AC2%E6%AC%A1%E3%83%88%E3%83%A9%E3%83%B3%E3%83%97%E6%94%BF%E6%A8%A9%E3%81%AE%E9%96%A2%E7%A8%8E#cite_note-18%22%5D
の”国別の「相互関税」の一覧（2025年4月2日時点）”の表　を参照文書とする）<br>
（ほぼGemini　2.5Flash（preview）版と対話しながら進めた。このHOMEWORK.mdも書いてもらったものを微修正。）

作成コード：
day3/day3_homework_250511_06_submission.ipynb

## 質問設計の観点と意図

ごく最近のことは、オープンソースのLLMモデルは知らないだろうと考え、トランプ関税に関することとし、具体的に関税の数値に関して多くの質問を設定した。質問は以下の5つとした。

1.  日本に課される「相互関税」の税率は何%ですか？
2.  記載されている国の中で、最も高い「相互関税」が課されているのはどの国ですか？またその税率は何%ですか？
3.  「相互関税」が10%とされている国はどこですか？
4.  このデータの時点は何時ですか？
5.  第二次トランプ政権で導入が検討されているとされる「普遍的な基礎関税」とは、どのような構想ですか？

参照文書：https://ja.wikipedia.org/wiki/%E7%AC%AC2%E6%AC%A1%E3%83%88%E3%83%A9%E3%83%B3%E3%83%97%E6%94%BF%E6%A8%A9%E3%81%AE%E9%96%A2%E7%A8%8E#cite_note-18%22%5D
の”国別の「相互関税」の一覧（2025年4月2日時点）”の表　をとする）<br>

これらの質問は、主に以下の観点から設計しました。

* **特定事実の抽出:** 質問1、2、3は、表データから特定の数値や国名を正確に抽出できるかを確認するもの。特に質問2は、単純な抽出だけでなく、表内のデータを比較して最大値を特定するという、より高度な処理が必要となる可能性があります。
* **メタ情報の抽出:** 質問4は、表の本体ではなく、表のキャプションや周囲のテキストに含まれる可能性のある情報（データの時点）をRAGが参照できるかを確認するもの。
* **参照範囲外の質問:** 質問5は、RAGに与えられた参照資料（表データ）には直接含まれていない、一般的な概念に関する質問です。これにより、RAGが参照資料の範囲外の質問に対してどのように振る舞うか、その限界を確認する意図がある。

これらの質問に対するベースラインモデル（RAGなし）とRAGモデルの回答を比較することで、RAGが特定の外部情報を用いたLLMの回答精度にどの程度貢献するのか、またどのような課題があるのかを検証することを意図した。

各質問と準備したgold answerは以下<br>
1."日本に課される「相互関税」の税率は何%ですか？": "表によると、日本に課される「相互関税」の税率は24%です。"<br>
2."記載されている国の中で、最も高い「相互関税」が課されているのはどの国ですか？またその税率は何%ですか？": "表によると、最も高い「相互関税」が課されているのはレソトで、その税率は50%です。", # 修正<br>
3."「相互関税」が10%とされている国はどこですか？": "表において、「相互関税」が10%とされているのはすべての国と地域です。", # 修正<br>
4."このデータの時点は何時ですか？": "表のデータは2025年4月2日時点の情報です。",<br>
5."第二次トランプ政権で導入が検討されているとされる「普遍的な基礎関税」とは、どのような構想ですか？": "普遍的な基礎関税とは、第二次トランプ政権が誕生した場合に、全ての輸入品に対して一律に関税を引き上げる構想です。国内産業の保護、貿易赤字の削減、公正な貿易条件の確立を目指しています。" # この回答は表データには含まれませんが、ベースラインとの比較のために残す。<br>
     

## RAGの実装方法と工夫点

RAGシステムの実装は、主に以下のステップで行った。

1.  **環境準備:** 必要なライブラリ（`transformers`, `bitsandbytes`, `accelerate`, `langchain-community`, `chromadb`, `sentence-transformers`, `openai`, `pandas`, `requests`, `beautifulsoup4`）をインストールし、Hugging Face Hubへのログイン設定を行いました。
2.  **LLMのロード:** 日本語に特化した軽量モデルである `google/gemma-2-2b-jpn-it` を4bit量子化を用いてロードしました。
3.  **参照資料の取得と処理:**
    * `requests`と`beautifulsoup4`を用いてWikipediaページのHTMLを取得・解析しました。
    * `pandas.read_html`でページ内の表を検出し、目的の表（列名に「国または地域」と「関税率」を含むもの）を特定しました。
    * 特定した表を文字列形式に変換しました。
    * **工夫点:** 表の本体だけでなく、`BeautifulSoup`を用いて表の`<caption>`要素を取得し、そのテキストも抽出しました。これにより、表のタイトルに含まれる「データの時点」のようなメタ情報もRAGの参照対象に含めるようにした。
    * 抽出した表データ文字列とキャプションテキストをLangChainの`Document`オブジェクトとして格納した。
4.  **テキスト分割（チャンク化）:** `RecursiveCharacterTextSplitter`を用いて、抽出したDocumentをチャンクに分割しました。今回は表全体とキャプションをまとめて大きなチャンクとして扱った。
5.  **エンベディングとベクトルストア構築:**
    * `sentence-transformers`ライブラリの`SentenceTransformerEmbeddings`を用いて、チャンクを数値ベクトルに変換するためのエンベディングモデル (`infly/inf-retriever-v1-1.5b`) をロードしました。以前の実行で発生したエラーを回避するため、`trust_remote_code=True`パラメータは削除した。
    * エンベディングされたチャンクをインメモリの`Chroma`ベクトルストアに格納した。
    * ベクトルストアから関連ドキュメントを取得するための`retriever`を設定した。
6.  **RAGによる回答生成:**
    * ユーザーからの質問に対して、`retriever`を用いてベクトルストアから関連性の高いチャンクを取得しました。
    * 取得したチャンクを質問とともにLLMへのプロンプトに含め、回答を生成しました。
7.  **評価:** OpenAI API (`gpt-4o-mini`) を用いて、生成された回答の正確性を評価しました（評価スコア0, 2, 4）。評価基準となるゴールドアンサーは、抽出した表データの内容に基づいて手動で作成・修正した。

## 結果の分析と考察

ベースラインモデル（RAGなし）とRAGモデルの回答および評価結果を比較し、RAGの有効性と限界について分析する。

### 実行結果の概要

**表の読み取り:**
* Wikipediaページから6個の表を検出し、目的の表（表2）を正しく特定・抽出できた。
* 表のキャプション「国別の「相互関税」の一覧（2025年4月2日時点）」も抽出され、追加情報としてRAGの参照対象に含まれた。

**ベースラインモデル（RAGなし）の回答と評価:**
* 質問1（日本への関税率）：回答できず（評価0）。一般的な関税の説明に留まった。
* 質問2（最も高い関税率）：回答できず（評価0）。リアルタイム情報へのアクセス不可を理由に挙げている。
* 質問3（10%関税の国）：具体的な国名を挙げられず（評価0）。情報の変化や複数国への適用に言及している。
* 質問4（データの時点）：回答できず（評価0）。データの時点を質問者に尋ね返している。
* 質問5（普遍的な基礎関税）：構想について一般的な説明を試みていますが、表データにはない情報のため正確性に欠け、評価は0であった。

ベースラインモデルは、学習データにない特定の事実情報（特に数値や固有名詞、日付）に関する質問には全く回答できていない。

**RAGモデルの回答と評価:**
* 質問1（日本への関税率）：**24%**と正確に回答（評価4）。
* 質問2（最も高い関税率）：**シリア 41%**と回答（評価0）。表データではレソト 50%が最高税率であり、不正確であった。
* 質問3（10%関税の国）：**すべての国と地域**と回答（評価2）。表データの内容（最下行）とは一致している。
* 質問4（データの時点）：**2025年4月2日**と正確に回答（評価4）。キャプション抽出の改善により、データの時点を特定できた。
* 質問5（普遍的な基礎関税）：**参考情報には含まれていません**と回答（評価0）。

### RAGありとRAGなしの差異分析

RAGを導入することで、ベースラインモデルでは全く回答できなかった表データに関する質問（質問1, 4）に対して、**具体的かつ正確な回答を生成できるようになりました**。特に質問4のように、表本体だけでなくキャプションなどの周辺情報も含めるようにデータ取得を改善したことで、回答精度が向上しました。

### RAGによって回答が改善したケースと悪化したケース

* **改善したケース:**
    * **質問1（日本への関税率）:** ベースラインでは回答不可だったものが、RAGにより表データから正確な数値（24%）を抽出・回答できるようになり、評価も満点となった。
    * **質問4（データの時点）:** ベースラインでは回答不可だったものが、キャプション抽出の改善により表題から正確な日付（2025年4月2日）を特定・回答できるようになり、評価も満点となった。

* **悪化したケース:**
    * 今回の実験では、ベースラインモデルが表データに関する質問にほぼ回答できなかったため、RAGによって回答が「悪化した」と明確に言えるケースはなかった。しかし、RAGモデルが不正確な回答を生成したケースはあります（質問2）。これはRAGの限界として捉えるべきと考えられる。

### RAGの有効性と限界についての考察

**RAGの有効性:**
* RAGは、LLMが学習データにない特定の事実情報（数値、固有名詞、日付など）に基づいて正確な回答を生成する能力を大幅に向上させる。今回の実験では、Wikipediaの表データという外部情報を活用することで、具体的な関税率やデータの時点に関する質問に正確に回答できるようになった。
* 参照資料の質と関連情報検索の精度が高ければ、LLMの「ハルシネーション」（事実に基づかない情報を生成すること）を抑制し、回答の信頼性を高めることが期待できる。

**RAGの限界:**
* **推論能力の限界:** 質問2のように、表データ内の情報を単純に抽出するだけでなく、比較や集計といった推論が必要な質問に対しては、RAGモデルが正確な回答を生成できない場合がある。これは、LLMが与えられた文脈をどのように解釈し、推論を行うかに依存する。
* **参照資料の範囲:** 質問5のように、RAGの参照資料に含まれていない情報に関する質問には回答でない。「参考情報には含まれていません」と回答できた点は、ハルシネーションを防ぐという意味では良い挙動と考えられるが、情報源の範囲が回答可能な範囲を限定するというRAGの根本的な限界を示している。
* **データ取得・処理の精度:** 質問4の初期の実行結果のように、参照資料から必要な情報を正確に抽出・整形できない場合、RAGは効果を発揮できない。表データの場合、構造化されていないテキスト情報（キャプションなど）も重要になるため、取得方法の工夫が必要である。
* **評価基準との整合性:** 質問3の例のように、RAGモデルが参照資料に忠実に回答しても、評価が低くなることがみられた。これはRAGシステム自体の問題というより、評価系構築の難しさを示唆していると考えられる。

## 発展的な改善案（任意）
* **自動評価:**  演習ipynbファイルのtemplate_accuracy1を入れる形で、openaiモデルでの評価を実施した。

